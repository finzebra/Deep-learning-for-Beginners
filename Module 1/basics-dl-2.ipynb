{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\n# Actual labels (Binary: 0 or 1)\ny_true = np.array([1, 0, 1, 1])\n\n# Predicted probabilities\ny_pred = np.array([0.9, 0.2, 0.8, 0.7])\n\n# Compute binary cross-entropy loss\nbce_loss = tf.keras.losses.BinaryCrossentropy()\nloss = bce_loss(y_true, y_pred).numpy()\n\nprint(\"Binary Cross-Entropy Loss:\", loss)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\n# True labels (one-hot encoded)\ny_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n# Predicted probabilities for 3 classes\ny_pred = np.array([[0.7, 0.2, 0.1], [0.1, 0.8, 0.1], [0.2, 0.3, 0.5]])\n\n# Compute categorical cross-entropy loss\ncce_loss = tf.keras.losses.CategoricalCrossentropy()\nloss = cce_loss(y_true, y_pred).numpy()\n\nprint(\"Categorical Cross-Entropy Loss:\", loss)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import SGD\n\n# Define a simple neural network model\nmodel = models.Sequential([\n    layers.Flatten(input_shape=(28, 28)),  # Flatten the input (e.g., 28x28 images)\n    layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n    layers.Dense(10, activation='softmax')  # Output layer for 10 classes (e.g., digits 0-9)\n])\n\n# Compile model using SGD optimizer\nmodel.compile(optimizer=SGD(learning_rate=0.01),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\n# Compile model using Adam optimizer\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop\n\n# Compile model using RMSprop optimizer\nmodel.compile(optimizer=RMSprop(learning_rate=0.001),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# Normalize the images (scale pixel values between 0 and 1)\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Flatten the images (28x28) to 1D (784)\nx_train = x_train.reshape(-1, 28 * 28)\nx_test = x_test.reshape(-1, 28 * 28)\n\n# Define the model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),  # Hidden layer with ReLU activation\n    tf.keras.layers.Dropout(0.2),  # Dropout to avoid overfitting\n    tf.keras.layers.Dense(10, activation='softmax')  # Output layer (10 classes for digits 0-9)\n])\n\n# Adam optimizer with a learning rate of 0.001\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n# Compile the model\nmodel.compile(optimizer=optimizer,\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model with a batch size of 32 for 10 epochs\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n\n# Plot training and validation accuracy\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adam optimizer with a learning rate of 0.001\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n# Compile the model\nmodel.compile(optimizer=optimizer,\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Training with a smaller learning rate\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using a batch size of 64\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the number of epochs\nepochs = 10  # You can change this number based on your needs\n\n# Train the model for the specified number of epochs\nhistory = model.fit(x_train, y_train, epochs=epochs, batch_size=32, validation_data=(x_test, y_test))\n\n# Plot training and validation accuracy\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\n# Define a function to build a Keras model\ndef create_model(learning_rate=0.001):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# Wrap the model for GridSearchCV\nmodel = KerasClassifier(build_fn=create_model, epochs=10, batch_size=64, verbose=0)\n\n# Define the parameter grid\nparam_grid = {'learning_rate': [0.001, 0.01, 0.1]}\n\n# Perform grid search\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid_result = grid.fit(x_train, y_train)\n\n# Print the best hyperparameters\nprint(f\"Best learning rate: {grid_result.best_params_['learning_rate']}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up an exponential learning rate decay\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.01,\n    decay_steps=100000,\n    decay_rate=0.96,\n    staircase=True\n)\n\n# Compile model with dynamic learning rate\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import layers, models\n\n# Define a simple model with batch normalization\nmodel = models.Sequential([\n    layers.Input(shape=(784,)),  # Use Input layer to specify input shape\n    layers.Dense(128, activation='relu'),\n    layers.BatchNormalization(),  # Apply batch normalization after the hidden layer\n    layers.Dropout(0.2),\n    layers.Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Summary of the model to verify\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import layers, initializers\n\n# Define a model using He initialization for ReLU activations\nmodel = tf.keras.Sequential([\n    layers.Dense(128, activation='relu', kernel_initializer=initializers.HeNormal(), input_shape=(784,)),\n    layers.BatchNormalization(),\n    layers.Dropout(0.2),\n    layers.Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Dropout\n\nmodel = tf.keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=(784,)),\n    Dropout(0.2),  # Dropout layer with 20% dropout\n    layers.Dense(10, activation='softmax')\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import regularizers\n\nmodel = tf.keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=(784,), kernel_regularizer=regularizers.l2(0.01)),\n    layers.Dense(10, activation='softmax')\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n# Define the early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n# Train the model with early stopping\nhistory = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test), callbacks=[early_stopping])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training and validation accuracy\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Plot training and validation loss\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, models\nfrom sklearn.model_selection import train_test_split\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# Normalize the data (scale pixel values between 0 and 1)\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Flatten the images (28x28) to 1D (784)\nx_train = x_train.reshape(-1, 28 * 28)\nx_test = x_test.reshape(-1, 28 * 28)\n\n# Split the training set into training and validation sets\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the model\nmodel = models.Sequential([\n    layers.Dense(128, activation='relu', input_shape=(784,)),  # Hidden layer with 128 neurons and ReLU activation\n    layers.Dropout(0.2),  # Dropout to prevent overfitting\n    layers.Dense(10, activation='softmax')  # Output layer with 10 classes for digits 0-9\n])\n\n# Compile the model with Adam optimizer and sparse categorical cross-entropy loss\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}