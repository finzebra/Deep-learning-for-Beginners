{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport yfinance as yf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, GRU, MultiHeadAttention, LayerNormalization\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\ntf.get_logger().setLevel('ERROR')\n\n## Data Preparation\n# Download S&P 500 data\ndef download_sp500_data(ticker='^GSPC', start_date='2010-01-01', end_date='2020-12-31'):\n    try:\n        data = yf.download(ticker, start=start_date, end=end_date)\n        if data.empty:\n            print(f\"Failed to download data for {ticker}. Please check your internet connection or ticker symbol.\")\n            return None\n        return data\n    except Exception as e:\n        print(f\"Error downloading data: {str(e)}\")\n        return None\n\n# Add technical indicators\ndef add_technical_indicators(df):\n    if df is None or df.empty:\n        return None\n    \n    try:\n        # Moving Averages\n        df['MA_10'] = df['Close'].rolling(window=10).mean()\n        df['MA_20'] = df['Close'].rolling(window=20).mean()\n        df['MA_50'] = df['Close'].rolling(window=50).mean()\n        \n        # RSI\n        delta = df['Close'].diff()\n        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n        # Avoid division by zero\n        loss = loss.replace(0, np.finfo(float).eps)\n        rs = gain / loss\n        df['RSI'] = 100 - (100 / (1 + rs))\n        \n        # MACD\n        exp12 = df['Close'].ewm(span=12, adjust=False).mean()\n        exp26 = df['Close'].ewm(span=26, adjust=False).mean()\n        df['MACD'] = exp12 - exp26\n        df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n        \n        # Bollinger Bands\n        df['Upper_Band'] = df['MA_20'] + (df['Close'].rolling(window=20).std() * 2)\n        df['Lower_Band'] = df['MA_20'] - (df['Close'].rolling(window=20).std() * 2)\n        \n        return df.dropna()\n    except Exception as e:\n        print(f\"Error adding technical indicators: {str(e)}\")\n        return None\n\n# Prepare data for time series prediction\ndef prepare_data(df, lookback=60, forecast_horizon=5):\n    if df is None or df.empty:\n        print(\"Cannot prepare data: DataFrame is empty or None\")\n        return None, None, None\n    \n    try:\n        # Select features to scale\n        features_to_scale = ['Close', 'Volume', 'MA_10', 'MA_20', 'MA_50', 'RSI', 'MACD', 'Signal_Line', 'Sentiment']\n        \n        # Check which features actually exist in the dataframe\n        available_features = [f for f in features_to_scale if f in df.columns]\n        print(f\"Using features: {available_features}\")\n        \n        scaler = MinMaxScaler(feature_range=(0, 1))\n        scaled_data = scaler.fit_transform(df[available_features])\n        \n        X, y = [], []\n        for i in range(lookback, len(scaled_data)-forecast_horizon):\n            X.append(scaled_data[i-lookback:i])\n            # For multi-day forecasts, we take 'forecast_horizon' days of the 'Close' price\n            y.append(scaled_data[i:i+forecast_horizon, 0])  # Assuming 'Close' is the first column\n            \n        return np.array(X), np.array(y), scaler\n    except Exception as e:\n        print(f\"Error preparing data: {str(e)}\")\n        return None, None, None\n\n# Load sentiment data (simulated)\ndef load_sentiment_data(start_date, end_date):\n    try:\n        dates = pd.date_range(start=start_date, end=end_date)\n        sentiment = np.random.uniform(-1, 1, size=len(dates))\n        return pd.DataFrame({'Date': dates, 'Sentiment': sentiment})\n    except Exception as e:\n        print(f\"Error loading sentiment data: {str(e)}\")\n        return pd.DataFrame()  # Return empty DataFrame on error\n\n# Define model architectures\ndef build_lstm_model(input_shape):\n    model = Sequential()\n    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n    model.add(Dropout(0.2))\n    model.add(LSTM(50, return_sequences=False))\n    model.add(Dropout(0.2))\n    model.add(Dense(5))  # Output layer (assuming forecast_horizon=5)\n    \n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n    return model\n\ndef build_cnn_model(input_shape):\n    model = Sequential()\n    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(5))  # Output layer (assuming forecast_horizon=5)\n    \n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n    return model\n\ndef build_transformer_model(input_shape):\n    inputs = tf.keras.Input(shape=input_shape)\n    \n    # Add positional encoding\n    x = inputs\n    \n    # Multi-head attention block\n    attention_output = MultiHeadAttention(\n        num_heads=4, key_dim=32)(x, x)\n    attention_output = Dropout(0.1)(attention_output)\n    x = LayerNormalization(epsilon=1e-6)(x + attention_output)\n    \n    # Feed-forward network\n    ffn = Sequential([\n        Dense(128, activation='relu'),\n        Dense(input_shape[-1])\n    ])\n    ffn_output = ffn(x)\n    ffn_output = Dropout(0.1)(ffn_output)\n    x = LayerNormalization(epsilon=1e-6)(x + ffn_output)\n    \n    # Flatten and output layer\n    x = Flatten()(x)\n    outputs = Dense(5)(x)  # Output layer (assuming forecast_horizon=5)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n    return model\n\n# Define Trading Environment for RL example\nclass TradingEnvironment:\n    def __init__(self, price_data, initial_balance=10000):\n        self.price_data = price_data\n        self.initial_balance = initial_balance\n        self.reset()\n        \n    def reset(self):\n        self.balance = self.initial_balance\n        self.position = 0  # 0 = no position, 1 = long\n        self.current_step = 0\n        self.total_steps = len(self.price_data)\n        return self._get_observation()\n        \n    def _get_observation(self):\n        if self.current_step >= self.total_steps:\n            return None\n        return self.price_data[self.current_step]\n        \n    def step(self, action):\n        # Action: 0 = hold, 1 = buy, 2 = sell\n        if self.current_step >= self.total_steps - 1:\n            return None, 0, True, {}\n            \n        reward = 0\n        # Current and next price (using close price)\n        current_price = self.price_data[self.current_step][0][-1][0]  # Last timepoint, close price\n        self.current_step += 1\n        next_price = self.price_data[self.current_step][0][-1][0]\n        \n        price_change = next_price - current_price\n        \n        # Execute action\n        if action == 1 and self.position == 0:  # Buy\n            self.position = 1\n            reward = price_change\n        elif action == 2 and self.position == 1:  # Sell\n            self.position = 0\n            reward = -price_change\n            \n        # If holding position, reward/penalty based on price movement\n        if self.position == 1:\n            reward = price_change\n            \n        done = self.current_step >= self.total_steps - 1\n        \n        return self._get_observation(), reward, done, {}\n\n# Evaluation function\ndef evaluate_model(model, X_test, y_test, scaler, feature_pos=0):\n    \"\"\"\n    Evaluate model predictions and calculate error metrics.\n    \n    Args:\n        model: Trained model\n        X_test: Test features\n        y_test: True values\n        scaler: Fitted scaler used for inverse transformation\n        feature_pos: Position of the target feature in the scaled data (default: 0 for 'Close')\n    \n    Returns:\n        mse: Mean squared error\n        y_true: Original scale true values\n        y_pred: Original scale predictions\n    \"\"\"\n    try:\n        # Get predictions\n        predictions = model.predict(X_test)\n        \n        # For multi-day forecasts, we'll only evaluate the first day for simplicity\n        first_day_preds = predictions[:, 0]\n        first_day_true = y_test[:, 0]\n        \n        # Create dummy arrays for inverse scaling\n        feature_count = len(scaler.feature_names_in_)\n        \n        dummy_pred = np.zeros((len(first_day_preds), feature_count))\n        dummy_pred[:, feature_pos] = first_day_preds\n        \n        dummy_true = np.zeros((len(first_day_true), feature_count))\n        dummy_true[:, feature_pos] = first_day_true\n        \n        # Inverse transform\n        y_pred = scaler.inverse_transform(dummy_pred)[:, feature_pos]\n        y_true = scaler.inverse_transform(dummy_true)[:, feature_pos]\n        \n        # Calculate MSE\n        mse = mean_squared_error(y_true, y_pred)\n        \n        return mse, y_true, y_pred\n    except Exception as e:\n        print(f\"Error in model evaluation: {str(e)}\")\n        return float('inf'), None, None\n\n## Main Execution\nif __name__ == \"__main__\":\n    print(\"Downloading and preparing data...\")\n    \n    # 1. Load and prepare data\n    sp500_data = download_sp500_data()\n    \n    if sp500_data is None or sp500_data.empty:\n        print(\"Failed to get S&P 500 data. Exiting.\")\n        exit()\n        \n    print(f\"Downloaded data shape: {sp500_data.shape}\")\n    \n    # Remove any NaT values in the index\n    sp500_data = sp500_data[sp500_data.index.notnull()]\n    \n    if sp500_data.empty:\n        print(\"No valid data after filtering. Exiting.\")\n        exit()\n    \n    # Add technical indicators\n    sp500_data = add_technical_indicators(sp500_data.copy())  # Make sure to use a copy\n    \n    if sp500_data is None or sp500_data.empty:\n        print(\"Failed to add technical indicators. Exiting.\")\n        exit()\n    \n    # Generate sentiment data with matching dates\n    try:\n        start_date = sp500_data.index.min().strftime('%Y-%m-%d')\n        end_date = sp500_data.index.max().strftime('%Y-%m-%d')\n        sentiment_data = load_sentiment_data(start_date, end_date)\n        \n        # Merge sentiment data - ensure we're using the index properly\n        sp500_data = sp500_data.reset_index()  # Convert index to column\n        sp500_data = sp500_data.merge(sentiment_data, left_on='Date', right_on='Date', how='left')\n        sp500_data['Sentiment'] = sp500_data['Sentiment'].fillna(0)\n        sp500_data.set_index('Date', inplace=True)\n    except Exception as e:\n        print(f\"Error processing dates or merging sentiment data: {str(e)}\")\n        # Add a default sentiment column if merging fails\n        sp500_data['Sentiment'] = 0\n    \n    # Prepare data for modeling\n    print(\"Preparing training data...\")\n    X, y, scaler = prepare_data(sp500_data)\n    \n    if X is None or y is None:\n        print(\"Failed to prepare data. Exiting.\")\n        exit()\n        \n    print(f\"Prepared data shapes - X: {X.shape}, y: {y.shape}\")\n    \n    # Split data\n    train_size = int(0.8 * len(X))\n    X_train, X_test = X[:train_size], X[train_size:]\n    y_train, y_test = y[:train_size], y[train_size:]\n    \n    # 2. Train models\n    print(\"\\nTraining LSTM model...\")\n    lstm_model = build_lstm_model(X_train.shape[1:])\n    lstm_history = lstm_model.fit(X_train, y_train, epochs=20, batch_size=32, \n                                validation_split=0.2, verbose=1)\n    \n    print(\"\\nTraining CNN model...\")\n    cnn_model = build_cnn_model(X_train.shape[1:])\n    cnn_history = cnn_model.fit(X_train, y_train, epochs=20, batch_size=32, \n                               validation_split=0.2, verbose=1)\n    \n    print(\"\\nTraining Transformer model...\")\n    transformer_model = build_transformer_model(X_train.shape[1:])\n    transformer_history = transformer_model.fit(X_train, y_train, epochs=20, batch_size=32, \n                                             validation_split=0.2, verbose=1)\n    \n    # 3. Evaluate models\n    print(\"\\nEvaluating models...\")\n    lstm_mse, lstm_true, lstm_pred = evaluate_model(lstm_model, X_test, y_test, scaler)\n    cnn_mse, cnn_true, cnn_pred = evaluate_model(cnn_model, X_test, y_test, scaler)\n    transformer_mse, transformer_true, transformer_pred = evaluate_model(transformer_model, X_test, y_test, scaler)\n    \n    print(f\"\\nLSTM MSE: {lstm_mse:.4f}\")\n    print(f\"CNN MSE: {cnn_mse:.4f}\")\n    print(f\"Transformer MSE: {transformer_mse:.4f}\")\n    \n    # 4. Plot results if data is available\n    if lstm_true is not None and lstm_pred is not None:\n        plt.figure(figsize=(15, 6))\n        plt.plot(lstm_true, label='Actual Price', alpha=0.7)\n        plt.plot(lstm_pred, label='LSTM Prediction', alpha=0.7)\n        plt.plot(cnn_pred, label='CNN Prediction', alpha=0.7)\n        plt.plot(transformer_pred, label='Transformer Prediction', alpha=0.7)\n        plt.title('Model Predictions vs Actual Prices')\n        plt.xlabel('Time')\n        plt.ylabel('Price')\n        plt.legend()\n        plt.show()\n    \n    # 5. Simplified RL example\n    print(\"\\nRunning simplified RL example...\")\n    try:\n        env = TradingEnvironment(X_train)\n        state = env.reset()\n        done = False\n        total_reward = 0\n        \n        if state is not None:\n            while not done:\n                action = np.random.choice([0, 1, 2])  # Random policy for demonstration\n                result = env.step(action)\n                if result is None:\n                    break\n                next_state, reward, done, _ = result\n                total_reward += reward\n                state = next_state\n                \n            print(f\"Random Policy Total Reward: {total_reward:.2f}\")\n        else:\n            print(\"Failed to initialize trading environment.\")\n    except Exception as e:\n        print(f\"Error in RL simulation: {str(e)}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:14:55.379069Z","iopub.execute_input":"2025-04-17T16:14:55.379673Z","iopub.status.idle":"2025-04-17T16:15:12.397409Z","shell.execute_reply.started":"2025-04-17T16:14:55.379647Z","shell.execute_reply":"2025-04-17T16:15:12.396457Z"}},"outputs":[{"name":"stderr","text":"2025-04-17 16:14:58.798785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744906499.044924      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744906499.119921      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading and preparing data...\n","output_type":"stream"},{"name":"stderr","text":"[*********************100%***********************]  1 of 1 completed\n","output_type":"stream"},{"name":"stdout","text":"Failed to download data for ^GSPC. Please check your internet connection or ticker symbol.\nFailed to get S&P 500 data. Exiting.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3698083234.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloaded data shape: {sp500_data.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;31m# Remove any NaT values in the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"],"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'shape'","output_type":"error"}],"execution_count":1}]}